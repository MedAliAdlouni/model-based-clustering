data.frame(
Model = model,
Nb_Initializations = nb_init,
Init_Method = init_method,
BIC = bic,
Loss = loss,
stringsAsFactors = FALSE
)
)
}
}
}
return(results)
}
# Parameters to test
models <- c("basic", "homoscedastic", "CEM", "SEM")
nb_initializations_list <- c(5, 10, 20)
init_methods <- c("kmeans", "random")
# Assuming X_tsne is your reduced data and cluster is the number of clusters
results <- run_em_with_parameters(X_tsne=X_tsne, cluster = 5, models, nb_initializations_list, init_methods)
?em_algorithm
run_em_with_parameters <- function(X_tsne, cluster, models, nb_initializations_list, init_methods) {
# Create a data frame to store the results
results <- data.frame(
Model = character(),
Nb_Initializations = numeric(),
Init_Method = character(),
BIC = numeric(),
Loss = numeric(),
stringsAsFactors = FALSE
)
# Loop over all combinations of parameters
for (model in models) {
for (nb_init in nb_initializations_list) {
for (init_method in init_methods) {
# Run the EM algorithm
em_result <- em_algorithm(
X = X_tsne,
K = cluster,
model = model,
max_iterations = 100,
nb_initializations = nb_init,
loss_treshold = 1e-3,
plot_losses = FALSE,
nb_iter_init_step = 10,
init_method = init_method
)
# Extract the BIC and final loss (last value in losses)
bic <- em_result$bic
loss <- tail(em_result$losses, 1)  # Get the final loss value
# Store the results
results <- rbind(
results,
data.frame(
Model = model,
Nb_Initializations = nb_init,
Init_Method = init_method,
BIC = bic,
Loss = loss,
stringsAsFactors = FALSE
)
)
}
}
}
return(results)
}
# Parameters to test
models <- c("basic", "homoscedastic", "CEM", "SEM")
nb_initializations_list <- c(5, 10, 20)
init_methods <- c("kmeans", "random")
models <- c("basic", "homoscedastic")
nb_initializations_list <- c(5, 10)
init_methods <- c("kmeans", "random")
# Assuming X_tsne is your reduced data and cluster is the number of clusters
results <- run_em_with_parameters(X_tsne = X_tsne, cluster = 5, models = models, nb_initializations_list = nb_initializations_list, init_methods = init_methods)
results$BIC
results$Model
results$Loss
# Initialize an empty list to store the loss values for each model
losses_list <- list()
# Run the EM algorithm for each model
for (model_type in models) {
result <- em_algorithm(
X = my_data,             # Your input data
K = 3,                   # Number of clusters
model = model_type,      # Set the model type (basic, homoscedastic, CEM, SEM)
nb_initializations = 10, # Number of initializations
max_iterations = 100,    # Max iterations
loss_treshold = 1e-6,    # Loss threshold for convergence
plot_losses = FALSE,     # We don't need to plot the loss inside the function
nb_iter_init_step = 10,  # Number of iterations for the initialization step
init_method = "random"   # Initialization method
)
# Store the loss values for this model
losses_list[[model_type]] <- result$losses
}
# Initialize an empty list to store the loss values for each model
losses_list <- list()
# Run the EM algorithm for each model
for (model_type in models) {
result <- em_algorithm(
X = X_tsne,             # Your input data
K = 3,                   # Number of clusters
model = model_type,      # Set the model type (basic, homoscedastic, CEM, SEM)
nb_initializations = 10, # Number of initializations
max_iterations = 100,    # Max iterations
loss_treshold = 1e-2,    # Loss threshold for convergence
plot_losses = FALSE,     # We don't need to plot the loss inside the function
nb_iter_init_step = 10,  # Number of iterations for the initialization step
init_method = "random"   # Initialization method
)
# Store the loss values for this model
losses_list[[model_type]] <- result$losses
}
# Plot the loss evolution for each model
plot(1:length(losses_list[[1]]), losses_list[[1]], type = "l", col = "blue",
xlab = "Iterations", ylab = "Loss", lwd = 2,
main = "Loss Evolution for Different GMM Models")
for (i in 2:length(models)) {
lines(1:length(losses_list[[i]]), losses_list[[i]], col = i, lwd = 2)
}
# Add a legend
legend("topright", legend = models, col = 1:length(models), lwd = 2)
models <- c("basic", "homoscedastic", "CEM", "SEM")
# Initialize an empty list to store the loss values for each model
losses_list <- list()
# Run the EM algorithm for each model
for (model_type in models) {
result <- em_algorithm(
X = X_tsne,             # Your input data
K = 3,                   # Number of clusters
model = model_type,      # Set the model type (basic, homoscedastic, CEM, SEM)
nb_initializations = 10, # Number of initializations
max_iterations = 100,    # Max iterations
loss_treshold = 1e-2,    # Loss threshold for convergence
plot_losses = FALSE,     # We don't need to plot the loss inside the function
nb_iter_init_step = 10,  # Number of iterations for the initialization step
init_method = "random"   # Initialization method
)
# Store the loss values for this model
losses_list[[model_type]] <- result$losses
}
# Plot the loss evolution for each model
plot(1:length(losses_list[[1]]), losses_list[[1]], type = "l", col = "blue",
xlab = "Iterations", ylab = "Loss", lwd = 2,
main = "Loss Evolution for Different GMM Models")
for (i in 2:length(models)) {
lines(1:length(losses_list[[i]]), losses_list[[i]], col = i, lwd = 2)
}
# Add a legend
legend("topright", legend = models, col = 1:length(models), lwd = 2)
# Initialize an empty list to store the loss values for each model
losses_list <- list()
# Run the EM algorithm for each model
for (init in nb_initializations_list) {
result <- em_algorithm(
X = X_tsne,             # Your input data
K = 3,                   # Number of clusters
model = "basic",      # Set the model type (basic, homoscedastic, CEM, SEM)
nb_initializations = init, # Number of initializations
max_iterations = 100,    # Max iterations
loss_treshold = 1e-6,    # Loss threshold for convergence
plot_losses = FALSE,     # We don't need to plot the loss inside the function
nb_iter_init_step = 20,  # Number of iterations for the initialization step
init_method = "random"   # Initialization method
)
# Store the loss values for this model
losses_list[[model_type]] <- result$losses
}
# Plot the loss evolution for each model
plot(1:length(losses_list[[1]]), losses_list[[1]], type = "l", col = "blue",
xlab = "Iterations", ylab = "Loss", lwd = 2,
main = "Loss Evolution for Different GMM Models")
for (i in 2:length(models)) {
lines(1:length(losses_list[[i]]), losses_list[[i]], col = i, lwd = 2)
}
# Initialize an empty list to store the loss values for each model
losses_list <- list()
# Run the EM algorithm for each model
for (init in nb_initializations_list) {
result <- em_algorithm(
X = X_tsne,             # Your input data
K = 3,                   # Number of clusters
model = "basic",      # Set the model type (basic, homoscedastic, CEM, SEM)
nb_initializations = init, # Number of initializations
max_iterations = 100,    # Max iterations
loss_treshold = 1e-6,    # Loss threshold for convergence
plot_losses = FALSE,     # We don't need to plot the loss inside the function
nb_iter_init_step = 20,  # Number of iterations for the initialization step
init_method = "random"   # Initialization method
)
# Store the loss values for this model
losses_list[[init]] <- result$losses
}
# Plot the loss evolution for each model
plot(1:length(losses_list[[1]]), losses_list[[1]], type = "l", col = "blue",
xlab = "Iterations", ylab = "Loss", lwd = 2,
main = "Loss Evolution for Different GMM Models")
for (i in 2:length(models)) {
lines(1:length(losses_list[[i]]), losses_list[[i]], col = i, lwd = 2)
}
# Add a legend
legend("topright", legend = models, col = 1:length(models), lwd = 2)
em_tsne_homoscedastic <- em_algorithm(X_tsne, K, model = "homoscedastic", max_iterations = 100, nb_initializations = 5,
loss_treshold = 1e-6, plot_losses = TRUE, nb_iter_init_step = 10, init_method = "kmeans")
em_tsne <- em_algorithm(X_tsne, K, model = "basic", max_iterations = 100, nb_initializations = 5,
loss_treshold = 1e-6, plot_losses = TRUE, nb_iter_init_step = 10, init_method = "kmeans")
# Extract the losses
losses_basic <- em_tsne$losses
losses_homoscedastic <- em_tsne_homoscedastic$losses
# Create a data frame to combine the losses from both models
losses_df <- data.frame(
Iteration = rep(1:length(losses_basic), 2),  # Combine iterations for both models
Loss = c(losses_basic, losses_homoscedastic),  # Combine losses for both models
Model = rep(c("basic", "homoscedastic"), each = length(losses_basic))  # Label for each model
)
# Extract the losses
losses_basic <- em_tsne$losses
losses_homoscedastic <- em_tsne_homoscedastic$losses
# Create a data frame to combine the losses from both models
losses_df <- data.frame(
Iteration = rep(1:length(losses_basic), 2),  # Combine iterations for both models
Loss = c(losses_basic, losses_homoscedastic),  # Combine losses for both models
Model = rep(c("basic", "homoscedastic"), each = length(losses_basic))  # Label for each model
)
# Run EM algorithm on t-SNE-reduced data
cat("Running EM on t-SNE-reduced data...\n")
em_tsne <- em_algorithm(X_tsne, K, model = "basic", max_iterations = 30, nb_initializations = 5,
loss_treshold = 1e-50, plot_losses = TRUE, nb_iter_init_step = 10, init_method = "kmeans")
em_tsne_homoscedastic <- em_algorithm(X_tsne, K, model = "homoscedastic", max_iterations = 30, nb_initializations = 5,
loss_treshold = 1e-50, plot_losses = TRUE, nb_iter_init_step = 10, init_method = "kmeans")
# Run EM algorithm on t-SNE-reduced data
cat("Running EM on t-SNE-reduced data...\n")
em_tsne <- em_algorithm(X_tsne, K, model = "basic", max_iterations = 50, nb_initializations = 5,
loss_treshold = 1e-50, plot_losses = TRUE, nb_iter_init_step = 10, init_method = "kmeans")
em_tsne_homoscedastic <- em_algorithm(X_tsne, K, model = "homoscedastic", max_iterations = 50, nb_initializations = 5,
loss_treshold = 1e-50, plot_losses = TRUE, nb_iter_init_step = 10, init_method = "kmeans")
# Extract the losses
losses_basic <- em_tsne$losses
losses_homoscedastic <- em_tsne_homoscedastic$losses
# Create a data frame to combine the losses from both models
losses_df <- data.frame(
Iteration = rep(1:length(losses_basic), 2),  # Combine iterations for both models
Loss = c(losses_basic, losses_homoscedastic),  # Combine losses for both models
Model = rep(c("basic", "homoscedastic"), each = length(losses_basic))  # Label for each model
)
# Plot both losses on the same plot using ggplot2
ggplot(losses_df, aes(x = Iteration, y = Loss, color = Model)) +
geom_line() +
labs(title = "Comparison of Losses for 'Basic' and 'Homoscedastic' Models",
x = "Iteration",
y = "Loss") +
theme_minimal() +
theme(legend.title = element_blank(), legend.position = "right")
# Extract the losses
losses_basic <- em_tsne$losses
losses_homoscedastic <- em_tsne_homoscedastic$losses
# Make sure both loss vectors are of the same length for plotting
max_iterations <- max(length(losses_basic), length(losses_homoscedastic))
# Pad the shorter vector with NA values if necessary
losses_basic <- c(losses_basic, rep(NA, max_iterations - length(losses_basic)))
losses_homoscedastic <- c(losses_homoscedastic, rep(NA, max_iterations - length(losses_homoscedastic)))
# Create a data frame to combine the losses from both models
losses_df <- data.frame(
Iteration = rep(1:max_iterations, 2),  # Combine iterations for both models
Loss = c(losses_basic, losses_homoscedastic),  # Combine losses for both models
Model = rep(c("basic", "homoscedastic"), each = max_iterations)  # Label for each model
)
# Plot both losses on the same plot using ggplot2
ggplot(losses_df, aes(x = Iteration, y = Loss, color = Model)) +
geom_line() +
labs(title = "Comparison of Losses for 'Basic' and 'Homoscedastic' Models",
x = "Iteration",
y = "Loss") +
theme_minimal() +
theme(legend.title = element_blank(), legend.position = "right")
cat("Running EM on t-SNE-reduced data...\n")
em_tsne <- em_algorithm(X_tsne, K, model = "basic", max_iterations = 50, nb_initializations = 5,
loss_treshold = 1e-5, plot_losses = TRUE, nb_iter_init_step = 10, init_method = "kmeans")
em_tsne_homoscedastic <- em_algorithm(X_tsne, K, model = "homoscedastic", max_iterations = 50, nb_initializations = 20,
loss_treshold = 1e-5, plot_losses = TRUE, nb_iter_init_step = 10, init_method = "kmeans")
# Extract the losses
losses_basic <- em_tsne$losses
losses_homoscedastic <- em_tsne_homoscedastic$losses
# Make sure both loss vectors are of the same length for plotting
max_iterations <- max(length(losses_basic), length(losses_homoscedastic))
# Pad the shorter vector with NA values if necessary
losses_basic <- c(losses_basic, rep(NA, max_iterations - length(losses_basic)))
losses_homoscedastic <- c(losses_homoscedastic, rep(NA, max_iterations - length(losses_homoscedastic)))
# Create a data frame to combine the losses from both models
losses_df <- data.frame(
Iteration = rep(1:max_iterations, 2),  # Combine iterations for both models
Loss = c(losses_basic, losses_homoscedastic),  # Combine losses for both models
Model = rep(c("basic", "homoscedastic"), each = max_iterations)  # Label for each model
)
# Plot both losses on the same plot using ggplot2
ggplot(losses_df, aes(x = Iteration, y = Loss, color = Model)) +
geom_line() +
labs(title = "Comparison of Losses for 'Basic' and 'Homoscedastic' Models",
x = "Iteration",
y = "Loss") +
theme_minimal() +
theme(legend.title = element_blank(), legend.position = "right")
em_tsne <- em_algorithm(X_tsne, K, model = "basic", max_iterations = 50, nb_initializations = 5,
loss_treshold = 1e-5, plot_losses = TRUE, nb_iter_init_step = 10, init_method = "kmeans")
em_tsne_homoscedastic <- em_algorithm(X_tsne, K, model = "basic", max_iterations = 50, nb_initializations = 20,
loss_treshold = 1e-5, plot_losses = TRUE, nb_iter_init_step = 10, init_method = "kmeans")
cat("BIC score for EM_PCA: ", em_pca$bic, "\n")
cat("BIC score for EM_tSNE: ", em_tsne$bic, "\n")
# Extract the losses
losses_basic <- em_tsne$losses
losses_homoscedastic <- em_tsne_homoscedastic$losses
# Make sure both loss vectors are of the same length for plotting
max_iterations <- max(length(losses_basic), length(losses_homoscedastic))
# Pad the shorter vector with NA values if necessary
losses_basic <- c(losses_basic, rep(NA, max_iterations - length(losses_basic)))
losses_homoscedastic <- c(losses_homoscedastic, rep(NA, max_iterations - length(losses_homoscedastic)))
# Create a data frame to combine the losses from both models
losses_df <- data.frame(
Iteration = rep(1:max_iterations, 2),  # Combine iterations for both models
Loss = c(losses_basic, losses_homoscedastic),  # Combine losses for both models
Model = rep(c("basic", "homoscedastic"), each = max_iterations)  # Label for each model
)
# Plot both losses on the same plot using ggplot2
ggplot(losses_df, aes(x = Iteration, y = Loss, color = Model)) +
geom_line() +
labs(title = "Comparison of Losses for 'Basic' and 'Homoscedastic' Models",
x = "Iteration",
y = "Loss") +
theme_minimal() +
theme(legend.title = element_blank(), legend.position = "right")
em_tsne_5_init <- em_algorithm(X_tsne, K, model = "basic", max_iterations = 100, nb_initializations = 5,
loss_treshold = 1e-6, plot_losses = FALSE, nb_iter_init_step = 10, init_method = "kmeans")
em_tsne_10_init <- em_algorithm(X_tsne, K, model = "basic", max_iterations = 100, nb_initializations = 10,
loss_treshold = 1e-6, plot_losses = FALSE, nb_iter_init_step = 10, init_method = "kmeans")
# Extract the losses for both initializations
losses_basic_5_init <- em_tsne_5_init$losses
losses_basic_10_init <- em_tsne_10_init$losses
# Make sure both loss vectors are of the same length for plotting
max_iterations <- max(length(losses_basic_5_init), length(losses_basic_10_init))
# Pad the shorter vector with NA values if necessary
losses_basic_5_init <- c(losses_basic_5_init, rep(NA, max_iterations - length(losses_basic_5_init)))
losses_basic_10_init <- c(losses_basic_10_init, rep(NA, max_iterations - length(losses_basic_10_init)))
# Create a data frame to combine the losses from both initializations
losses_df <- data.frame(
Iteration = rep(1:max_iterations, 2),  # Combine iterations for both initializations
Loss = c(losses_basic_5_init, losses_basic_10_init),  # Combine losses for both initializations
Init_Method = rep(c("5 Initializations", "10 Initializations"), each = max_iterations)  # Label for each initialization
)
# Plot losses for different initializations on the same plot
ggplot(losses_df, aes(x = Iteration, y = Loss, color = Init_Method)) +
geom_line() +
labs(title = "Comparison of Losses for Different Initializations",
x = "Iteration",
y = "Loss") +
theme_minimal() +
theme(legend.title = element_blank(), legend.position = "right")
em_tsne_5_init <- em_algorithm(X_tsne, K, model = "basic", max_iterations = 50, nb_initializations = 5,
loss_treshold = 1e-6, plot_losses = FALSE, nb_iter_init_step = 10, init_method = "random")
em_tsne_10_init <- em_algorithm(X_tsne, K, model = "basic", max_iterations = 50, nb_initializations = 30,
loss_treshold = 1e-6, plot_losses = FALSE, nb_iter_init_step = 10, init_method = "random")
# Extract the losses for both initializations
losses_basic_5_init <- em_tsne_5_init$losses
losses_basic_10_init <- em_tsne_10_init$losses
# Make sure both loss vectors are of the same length for plotting
max_iterations <- max(length(losses_basic_5_init), length(losses_basic_10_init))
# Pad the shorter vector with NA values if necessary
losses_basic_5_init <- c(losses_basic_5_init, rep(NA, max_iterations - length(losses_basic_5_init)))
losses_basic_10_init <- c(losses_basic_10_init, rep(NA, max_iterations - length(losses_basic_10_init)))
# Create a data frame to combine the losses from both initializations
losses_df <- data.frame(
Iteration = rep(1:max_iterations, 2),  # Combine iterations for both initializations
Loss = c(losses_basic_5_init, losses_basic_10_init),  # Combine losses for both initializations
Init_Method = rep(c("5 Initializations", "10 Initializations"), each = max_iterations)  # Label for each initialization
)
# Plot losses for different initializations on the same plot
ggplot(losses_df, aes(x = Iteration, y = Loss, color = Init_Method)) +
geom_line() +
labs(title = "Comparison of Losses for Different Initializations",
x = "Iteration",
y = "Loss") +
theme_minimal() +
theme(legend.title = element_blank(), legend.position = "right")
# Load required libraries
library(R6)
library(caret)
library(MyGMMPackage)
# Source the GMM class
# source("EM_algorithm_.R")
# Assign X and y from the iris dataset
X <- as.matrix(iris[, -5])  # Features (columns 1 to 4)
y <- iris[, 5]  # Target variable (Species)
true_labels <- as.numeric(y)  # Convert factor levels to numeric
# Standardize the features before passing to the EM algorithm
preprocess_params <- preProcess(X, method = c("center", "scale"))
X <- predict(preprocess_params, X)
# Split data into training and testing sets
set.seed(123)
train_index <- createDataPartition(y, p = 0.8, list = FALSE)  # 80% training, 20% testing
X_train <- X[train_index, ]
y_train <- true_labels[train_index]
X_test <- X[-train_index, ]
y_test <- true_labels[-train_index]
# Set parameters for the EM algorithm
K <- 3
model_types <- c("basic", "homoscedastic", "CEM", "SEM")
# Call EM algorithm for the training set
results <- em_algorithm(X_train, K, model = "basic", max_iterations = 50, nb_initializations = 5,
loss_treshold = 1e-6, plot_losses = TRUE, nb_iter_init_step = 10, init_method = "random")
# Extract results
losses <- results$losses
proportions <- results$proportions
mu <- results$mu
sigma <- results$sigma
last_t_i_k <- results$last_t_i_k
bic_score <- results$bic
maximum_a_posteriori <- results$maximum_a_posteriori
# Print results
print(last_t_i_k)
print(losses)
print(maximum_a_posteriori)
cat("BIC score: ", bic_score, "\n")  # Print BIC score
# Evaluate training performance scores
train_perf <- evaluate_gmm(y_train, maximum_a_posteriori)
train_confusion_matrix <- train_perf$confusion_matrix
train_accuracy <- train_perf$accuracy
cat("Training Confusion Matrix: \n")
print(train_confusion_matrix)
cat("Training Accuracy: ", train_accuracy * 100, "%\n")
# Evaluate test performance scores
# Predict cluster assignments for test data
test_clusters <- predict_gmm(X_test, proportions, mu, sigma, K)  # Ensure you have a function to predict clusters
test_perf <- evaluate_gmm(y_test, test_clusters)
test_confusion_matrix <- test_perf$confusion_matrix
test_accuracy <- test_perf$accuracy
cat("Testing Confusion Matrix: \n")
print(test_confusion_matrix)
cat("Testing Accuracy: ", test_accuracy * 100, "%\n")
# Load required libraries
library(caret)   # For standardization
library(Rtsne)   # For t-SNE
library(R6)
library(MyGMMPackage)
load("data/mnist.RData")
# Split data into training and testing sets
set.seed(123)
mnist_ytrain <-  as.numeric(mnist_ytrain)
# Standardize the data
# X_scaled <- scale(mnist_Xtrain[1:100, ], center = TRUE, scale = FALSE)
# Set parameters for the EM algorithm and the number of components
K <- 10  # Number of clusters
# Apply PCA
pca_result <- prcomp(mnist_Xtrain)
reduced_train <- pca_result$x[,1:50]
X_pca <- scale(reduced_train)
explained_variance_ratio <- (pca_result$sdev^2) / sum(pca_result$sdev^2)
total_variance_explained <- sum(explained_variance_ratio[1:50])
cat("Total variance explained by the first 50 components:", total_variance_explained, "\n")
# Apply t-SNE
X_tsne <- Rtsne(mnist_Xtrain, dims = 3, perplexity = 30, theta = 0.5)
X_tsne <- scale(X_tsne$Y)
# Assuming em_tsne$losses is the loss over iterations for each K
bics <- numeric(15)  # Assuming you test from K = 1 to K = 10
# Run EM algorithm on data
#em <- em_algorithm(X_scaled, K, model = "homoscedastic", max_iterations = 100, nb_initializations = 5,
#                       loss_treshold = 1e-6, plot_losses = TRUE, nb_iter_init_step = 10, init_method = "kmeans")
# Run EM algorithm on PCA-reduced data
cat("Running EM on PCA-reduced data...\n")
em_pca <- em_algorithm(X_pca, K, model = "basic", max_iterations = 100, nb_initializations = 5,
loss_treshold = 1e-6, plot_losses = TRUE, nb_iter_init_step = 10, init_method = "kmeans")
# Run EM algorithm on t-SNE-reduced data
cat("Running EM on t-SNE-reduced data...\n")
em_tsne <- em_algorithm(X_tsne, K, model = "basic", max_iterations = 100, nb_initializations = 5,
loss_treshold = 1e-6, plot_losses = TRUE, nb_iter_init_step = 10, init_method = "kmeans")
cat("BIC score for EM_PCA: ", em_pca$bic, "\n")
cat("BIC score for EM_tSNE: ", em_tsne$bic, "\n")
# Extract the losses from the EM algorithm results
losses_pca <- em_pca$losses
losses_tsne <- em_tsne$losses
# Determine the range of iterations for plotting
iterations_pca <- seq_along(losses_pca)
iterations_tsne <- seq_along(losses_tsne)
# Set up a 1-row, 2-column plotting layout
par(mfrow = c(1, 2))
# Plot the losses for EM on PCA-reduced data
plot(iterations_pca, losses_pca, type = "l", col = "blue", lwd = 2,
xlab = "Iterations", ylab = "Loss", main = "EM on PCA-Reduced Data")
# Plot the losses for EM on t-SNE-reduced data
plot(iterations_tsne, losses_tsne, type = "l", col = "red", lwd = 2,
xlab = "Iterations", ylab = "Loss", main = "EM on t-SNE-Reduced Data")
# Reset the plotting layout to default
par(mfrow = c(1, 1))
# Evaluate training performance scores
# TSNE
train_perf_tsne <- evaluate_gmm(mnist_ytrain, em_tsne$maximum_a_posteriori)
train_confusion_matrix_tsne <- train_perf_tsne$confusion_matrix
train_accuracy_tsne <- train_perf_tsne$accuracy
cat("Training Confusion Matrix: \n")
print(train_confusion_matrix_tsne)
cat("Training Accuracy: ", train_accuracy_tsne * 100, "%\n")
#PCA
train_perf_pca <- evaluate_gmm(mnist_ytrain, em_pca$maximum_a_posteriori)
train_confusion_matrix_pca <- train_perf_pca$confusion_matrix
train_accuracy_pca <- train_perf_pca$accuracy
cat("Training Confusion Matrix: \n")
print(train_confusion_matrix_pca)
cat("Training Accuracy: ", train_accuracy_pca * 100, "%\n")
# Evaluate test performance scores
# Predict cluster assignments for test data
#test_clusters <- predict_gmm(X_pca_test, em_tsne$proportions, em_tsne$mu, em_tsne$sigma, K)  # Ensure you have a function to predict clusters
#test_perf <- evaluate_gmm(ytest, test_clusters)
